# Data-Science-Assignments
This repository contains Assingments from my Data Science course.

It contains twenty Assignments.

## Table of Contents
- [Basic Statistics Level 1](#basic-statistics-level-1)
- [Basic Stats 2](#basic-stats-2)
- [Basics of Python](#basics-of-python)
- [Hypothesis Testing](#hypothesis-testing)
- [Exploratory Data Analysis 1 (EDA1)](#exploratory-data-analysis-1-eda1)
- [Multiple Linear Regression (MLR)](#multiple-linear-regression-mlr)
- [Logistic Regression](#logistic-regression)
- [Clustering](#clustering)
- [Principal Component Analysis (PCA)](#principal-component-analysis-pca)
- [Association Rules](#association-rules)
- [Recommendation System](#recommendation-system)
- [Exploratory Data Analysis 2 (EDA2)](#exploratory-data-analysis-2-eda2)
- [Decision Tree](#decision-tree)
- [Random Forest](#random-forest)
- [XGBoost & LightGBM (XGBM & LGBM)](#xgboost-lightgbm-xgbm-lgbm)
- [K-Nearest Neighbors (KNN)](#k-nearest-neighbors-knn)
- [Support Vector Machines (SVM)](#support-vector-machines-svm)
- [Neural Networks](#neural-networks)
- [Naive Bayes and Text Mining](#naive-bayes-and-text-mining)
- [Time Series](#time-series)

## Basic Statistics Level 1
- **Objective**: Learn basic concepts of descriptive statistics, such as mean, median, mode, variance, and standard deviation.
- **Approach**: Used Python libraries like NumPy and pandas to calculate and visualize statistical measures.

## Basic Stats 2
- **Objective**: Learn about probability distributions, statistical tests, and confidence intervals.
- **Approach**: Implemented probability functions and hypothesis tests using scipy and statsmodels.

## Basics of Python
- **Objective**: Strengthen Python programming skills including data structures, functions, and object-oriented programming.
- **Approach**: Created Python scripts that handle basic data manipulation and visualization.

## Hypothesis Testing
- **Objective**: Learn the concepts of null and alternative hypotheses, p-values, t-tests, and chi-square tests.
- **Approach**: Conducted various statistical tests to validate assumptions about the data.

## Exploratory Data Analysis 1 (EDA1)
- **Objective**: Perform initial data exploration to understand the distribution and relationships of the variables.
- **Approach**: Used pandas for data wrangling and matplotlib/seaborn for visualizations.

## Multiple Linear Regression (MLR)
- **Objective**: Build a regression model to predict a dependent variable using multiple independent variables.
- **Approach**: Implemented MLR using scikit-learn and evaluated model performance using R-squared and adjusted R-squared.

## Logistic Regression
- **Objective**: Learn how to model binary classification problems using logistic regression.
- **Approach**: Used logistic regression to predict binary outcomes and evaluated model performance.

## Clustering
- **Objective**: Group data points into clusters based on similarity using unsupervised learning techniques.
- **Approach**: Implemented clustering algorithms like K-means and Hierarchical clustering.

## Principal Component Analysis (PCA)
- **Objective**: Reduce dimensionality and improve model efficiency using PCA.
- **Approach**: Applied PCA to datasets and visualized the transformed data.

## Association Rules
- **Objective**: Discover relationships between variables using association rule mining.
- **Approach**: Used the Apriori algorithm to find frequent itemsets and association rules.

## Recommendation System
- **Objective**: Build a recommendation system based on user preferences and item similarities.
- **Approach**: Implemented collaborative filtering and content-based filtering methods.

## Exploratory Data Analysis 2 (EDA2)
- **Objective**: Dive deeper into data by handling missing values, outliers, and feature engineering.
- **Approach**: Cleaned and preprocessed data, visualized distributions, and identified key insights.

## Decision Tree
- **Objective**: Use decision trees for classification and regression problems.
- **Approach**: Built decision trees with scikit-learn, visualized them, and evaluated their performance.

## Random Forest
- **Objective**: Use ensemble methods like Random Forest to improve model accuracy.
- **Approach**: Implemented Random Forest and compared its performance to a single decision tree.

## XGBoost & LightGBM (XGBM & LGBM)
- **Objective**: Improve model performance using powerful gradient boosting techniques.
- **Approach**: Applied XGBoost and LightGBM models for classification and regression tasks.

## K-Nearest Neighbors (KNN)
- **Objective**: Use KNN for classification tasks by finding the closest data points in the feature space.
- **Approach**: Implemented KNN and evaluated its performance with cross-validation.

## Support Vector Machines (SVM)
- **Objective**: Use SVM to classify data by finding the optimal hyperplane that separates the classes.
- **Approach**: Applied SVM for both linear and non-linear classification problems.

## Neural Networks
- **Objective**: Learn about neural networks and their applications in machine learning.
- **Approach**: Built simple neural networks using TensorFlow/Keras for classification tasks.

## Naive Bayes and Text Mining
- **Objective**: Implement Naive Bayes for text classification tasks, such as spam detection.
- **Approach**: Preprocessed text data and applied the Naive Bayes algorithm for classification.

## Time Series
- **Objective**: Learn to analyze time-dependent data and make forecasts.
- **Approach**: Applied time series analysis techniques, including ARIMA and seasonal decomposition, to predict future values.

## Technologies Used
- **Python**: Used for data analysis, machine learning, and visualization.
- **Libraries**: pandas, NumPy, scikit-learn, matplotlib, seaborn, TensorFlow, LightGBM, XGBoost, statsmodels, and more.
- **Jupyter Notebooks**: Used for documenting the work and displaying results.

